{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "document_text = [\"The product is great...!\",\n",
    "    \"I'm not satisfied with the product\",\n",
    "    \"It's an average and regular product, nothing more special\"]\n",
    "\n",
    "stopWord = set(stopwords.words('english'))\n",
    "def pre_process(text):\n",
    "    tokens_01 = re.findall(r'\\w+', text.lower())\n",
    "    tokens_01 = [word for word in tokens_01 if word not in stopWord]\n",
    "    return tokens_01\n",
    "\n",
    "processed_doc = [pre_process(doc) for doc in document_text]\n",
    "dictionary = Dictionary(processed_doc)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_doc]\n",
    "coherence_scores = []\n",
    "for k in range(2, 11):\n",
    "    lda_model = LdaModel(corpus, num_topics=k, id2word=dictionary, passes=15)\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=processed_doc, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "optimal_k = coherence_scores.index(max(coherence_scores)) + 2  # +2 because we started from K=2\n",
    "print(f\"The number of optimal topics is: {optimal_k}\")\n",
    "optimal_lda_model = LdaModel(corpus, num_topics=optimal_k, id2word=dictionary, passes=15)\n",
    "topics = optimal_lda_model.print_topics(num_words=10)  # Adjusting the number of words as needed\n",
    "\n",
    "for t in topics:\n",
    "    print(t)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pyLDAvis in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.11.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: funcy in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: gensim in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (68.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from gensim->pyLDAvis) (2.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Requirement already satisfied: pyfume in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.2.25)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: simpful in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.11.0)\n",
      "Requirement already satisfied: fst-pso in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: miniful in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: great, satisfied, regular, average, special\n",
      "Topic 2: satisfied, special, regular, average, great\n",
      "Topic 3: special, regular, average, great, satisfied\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "req_documents = [\"The product is great...!\",\n",
    "    \"I'm not satisfied with the product\",\n",
    "    \"It's an average and regular product, nothing more special\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=5000, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(req_documents)\n",
    "\n",
    "num_topics = 4\n",
    "lsa = TruncatedSVD(n_components=num_topics)\n",
    "lsa_topic_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "singular_values = lsa.singular_values_\n",
    "\n",
    "for i, singular_value in enumerate(singular_values):\n",
    "    top_terms = [terms[j] for j in np.argsort(lsa.components_[i])[::-1][:10]]\n",
    "    print(f\"Topic {i+1}: {', '.join(top_terms)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bhanuprasadkommula/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "(0, '0.020*\"the\" + 0.016*\"google\" + 0.016*\"of\" + 0.016*\"search\" + 0.015*\"surgery-related\"')\n",
      "(1, '0.055*\",\" + 0.048*\"and\" + 0.027*\"marketing\" + 0.027*\"trends\" + 0.026*\"optimize\"')\n",
      "(2, '0.051*\"the\" + 0.044*\"search\" + 0.042*\"google\" + 0.037*\"of\" + 0.026*\"impact\"')\n",
      "(3, '0.044*\",\" + 0.038*\"and\" + 0.031*\".\" + 0.030*\"a\" + 0.023*\"search\"')\n",
      "(4, '0.042*\",\" + 0.030*\"increase\" + 0.026*\"and\" + 0.022*\"by\" + 0.022*\"understanding\"')\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "nltk.download('punkt')\n",
    "req_documents = ['By analyzing Google search data using Google Trends, we measured the impact of highly publicized plastic surgery-related events on the interest level of the general population in specific search terms.',\n",
    "             'Additionally, we investigated seasonal and geographic trends around interest in rhinoplasties, which is information that physicians and small surgical centers can use to optimize marketing decisions.',\n",
    "             'A noticeable impact was observed in both celebrity cases on search term volume, and a seasonal effect is apparent for rhinoplasty searches. ',\n",
    "             'As many surgeons already employ aggressive Internet marketing strategies, understanding and utilizing these trends could help optimize their investments, increase social engagement, and increase practice awareness by potential patients.']\n",
    "\n",
    "tokenizedDocs = [word_tokenize(doc.lower()) for doc in req_documents]\n",
    "\n",
    "dictionary = Dictionary(tokenizedDocs)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenizedDocs]\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, tokenizedDocs, limit, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=tokenizedDocs, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus, tokenizedDocs, limit=10)\n",
    "\n",
    "optimal_model = model_list[coherence_values.index(max(coherence_values))]\n",
    "optimal_K = optimal_model.num_topics\n",
    "def summarize_topics(model):\n",
    "    topics = model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "summarize_topics(optimal_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: bertopic in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (1.24.3)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (0.8.33)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (0.5.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (4.65.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from bertopic) (5.9.0)\n",
      "Requirement already satisfied: cython<3,>=0.27 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.32.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.0)\n",
      "Requirement already satisfied: nltk in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from umap-learn>=0.5.0->bertopic) (0.57.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
      "Requirement already satisfied: filelock in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.4.0)\n",
      "Requirement already satisfied: requests in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.40.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.3.2)\n",
      "Requirement already satisfied: click in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bhanuprasadkommula/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e76776d85b422e9ea30c3eaa5b0a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 22:50:17,846 - BERTopic - Transformed documents to Embeddings\n",
      "2023-11-05 22:50:21,992 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 22:51:17,869 - BERTopic - Clustered reduced embeddings\n",
      "2023-11-05 22:51:24,057 - BERTopic - Reduced number of topics from 366 to 249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: game, team, he, games, players (Freq: 1586)\n",
      "Topic 1: clipper, president, fbi, chip, government (Freq: 716)\n",
      "Topic 2: god, jesus, that, you, is (Freq: 649)\n",
      "Topic 3: gun, guns, militia, weapons, firearms (Freq: 395)\n",
      "Topic 4: israel, israeli, jews, arab, palestinian (Freq: 386)\n",
      "Topic 5: drive, drives, disk, mhz, hard (Freq: 321)\n",
      "Topic 6: homosexuality, homosexual, gay, homosexuals, sex (Freq: 220)\n",
      "Topic 7: turkish, armenian, armenians, armenia, were (Freq: 200)\n",
      "Topic 8: radar, detector, detectors, ir, tempest (Freq: 187)\n",
      "Topic 9: window, cursor, xterm, colormap, expose (Freq: 187)\n",
      "Topic 10: windows, dos, nt, memory, 31 (Freq: 173)\n",
      "Topic 11: drivers, card, diamond, driver, ati (Freq: 171)\n",
      "Topic 12: address, internet, email, organization, mail (Freq: 167)\n",
      "Topic 13: car, mustang, ford, toyota, convertible (Freq: 147)\n",
      "Topic 14: sale, cds, cd, shipping, speakers (Freq: 120)\n",
      "Topic 15: moon, billion, lunar, space, prize (Freq: 119)\n",
      "Topic 16: sky, space, billboard, vandalizing, advertising (Freq: 118)\n",
      "Topic 17: serial, port, modem, ports, board (Freq: 113)\n",
      "Topic 18: scsi, ide, scsi2, scsi1, controller (Freq: 104)\n",
      "Topic 19: modem, modems, fax, courier, baud (Freq: 102)\n",
      "Topic 20: bike, bikes, motorcycle, motorcycles, dod (Freq: 97)\n",
      "Topic 21: widget, motif, window, windows, server (Freq: 97)\n",
      "Topic 22: space, venus, mars, shuttle, lunar (Freq: 95)\n",
      "Topic 23: simms, simm, vram, 256k, quadra (Freq: 88)\n",
      "Topic 24: hell, eternal, god, heaven, jesus (Freq: 82)\n",
      "Topic 25: microsoft, os2, challenge, supporters, windows (Freq: 80)\n",
      "Topic 26: jpeg, gif, image, format, files (Freq: 77)\n",
      "Topic 27: 171292, 101292, pain, doctor, hernia (Freq: 73)\n",
      "Topic 28: undefined, symbol, libxmulibxmuso, gcc, error (Freq: 66)\n",
      "Topic 29: monitor, vga, monitors, horizontal, resolution (Freq: 65)\n",
      "Topic 30: games, sega, genesis, snes, game (Freq: 65)\n",
      "Topic 31: stephanopoulos, bosnia, serbs, mr, bosnian (Freq: 62)\n",
      "Topic 32: 3d, graphics, library, adobe, visualization (Freq: 61)\n",
      "Topic 33: bmw, moa, grips, joeridercactusorg, bmws (Freq: 59)\n",
      "Topic 34: morality, moral, keith, behavior, animals (Freq: 59)\n",
      "Topic 35: mary, her, she, immaculate, conception (Freq: 59)\n",
      "Topic 36: copy, protected, protection, disks, program (Freq: 58)\n",
      "Topic 37: msg, food, sensitivity, chinese, superstition (Freq: 58)\n",
      "Topic 38: rushdie, islam, islamic, gregg, khomeini (Freq: 54)\n",
      "Topic 39: shift, shifting, manual, automatic, clutch (Freq: 52)\n",
      "Topic 40: tax, taxes, income, vat, deficit (Freq: 52)\n",
      "Topic 41: government, libertarians, hendricks, stevehthoriscbrcom, libertarian (Freq: 51)\n",
      "Topic 42: amp, voltage, amps, generator, power (Freq: 51)\n",
      "Topic 43: mouse, driver, jumpy, mousecom, motion (Freq: 50)\n",
      "Topic 44: science, methodology, scientific, homeopathy, sasghmtheseusunxsascom (Freq: 50)\n",
      "Topic 45: hst, mission, servicing, reboost, shuttle (Freq: 50)\n",
      "Topic 46: gamma, bursters, oort, ray, cloud (Freq: 50)\n",
      "Topic 47: dog, dogs, bike, attack, boom (Freq: 49)\n",
      "Topic 48: battery, batteries, concrete, acid, lead (Freq: 49)\n",
      "Topic 49: cruel, punishment, penalty, death, capital (Freq: 49)\n",
      "Topic 50: printer, laserwriter, printers, print, deskjet (Freq: 48)\n",
      "Topic 51: v4, v8, v12, v6, vx (Freq: 46)\n",
      "Topic 52: books, bible, scripture, canon, sirach (Freq: 45)\n",
      "Topic 53: helmet, helmets, shoei, jacket, fit (Freq: 45)\n",
      "Topic 54: oil, drain, changing, plug, self (Freq: 44)\n",
      "Topic 55: love, petchgvg47gvgtekcom, god, daily, psalm (Freq: 43)\n",
      "Topic 56: jewish, baseball, players, lowenstein, koufax (Freq: 43)\n",
      "Topic 57: candida, yeast, systemic, bloom, infections (Freq: 43)\n",
      "Topic 58: drugs, drug, cocaine, marijuana, illegal (Freq: 43)\n",
      "Topic 59: cpu, fan, heat, sink, fans (Freq: 42)\n",
      "Topic 60: mac, disks, 800k, read, disk (Freq: 42)\n",
      "Topic 61: water, habitable, atmosphere, phd, planets (Freq: 41)\n",
      "Topic 62: countersteering, bike, countersteeringfaq, lean, technique (Freq: 40)\n",
      "Topic 63: marriage, married, marry, church, ceremony (Freq: 39)\n",
      "Topic 64: sphere, points, den, radius, bezier (Freq: 39)\n",
      "Topic 65: solar, spacecraft, mission, pluto, sail (Freq: 39)\n",
      "Topic 66: insurance, health, private, care, doctors (Freq: 39)\n",
      "Topic 67: xv, bit, image, images, 24bit (Freq: 38)\n",
      "Topic 68: baptism, sin, aaron, infants, original (Freq: 38)\n",
      "Topic 69: celp, speech, voice, compression, gtoalgtoalcom (Freq: 38)\n",
      "Topic 70: photography, krillean, kirlian, pictures, leaf (Freq: 38)\n",
      "Topic 71: monitor, screen, problem, video, 610 (Freq: 37)\n",
      "Topic 72: postscript, ghostscript, print, printer, file (Freq: 37)\n",
      "Topic 73: polygon, polygons, routine, convex, fast (Freq: 37)\n",
      "Topic 74: monitors, nanao, nec, 17, 5fg (Freq: 37)\n",
      "Topic 75: monitors, hours, 24, day, power (Freq: 37)\n",
      "Topic 76: fonts, font, atm, truetype, tt (Freq: 36)\n",
      "Topic 77: newsgroup, split, group, ch, aspects (Freq: 35)\n",
      "Topic 78: cancer, jb, treatment, placebo, patients (Freq: 35)\n",
      "Topic 79: list, ca, sale, guide, manuals (Freq: 34)\n",
      "Topic 80: phone, number, line, dial, operator (Freq: 34)\n",
      "Topic 81: ticket, airline, hotel, tickets, voucher (Freq: 33)\n",
      "Topic 82: kuwait, saudi, iraq, arabia, gulf (Freq: 32)\n",
      "Topic 83: images, ftp, data, image, otis (Freq: 32)\n",
      "Topic 84: slip, ashok, packet, winqvtnet, driver (Freq: 32)\n",
      "Topic 85: barbecued, carcinogenic, food, foods, taste (Freq: 31)\n",
      "Topic 86: wave, bikers, waving, squids, cage (Freq: 31)\n",
      "Topic 87: insurance, car, nofault, rates, deductible (Freq: 30)\n",
      "Topic 88: king, black, kyle, adjective, kkoppuxhcsouiucedu (Freq: 29)\n",
      "Topic 89: ftp, 00, radiosity, tcpdump, tcpview (Freq: 29)\n",
      "Topic 90: sleep, prozac, hismanal, antihistamines, antihistamine (Freq: 28)\n",
      "Topic 91: 1st, wolverine, comics, hulk, art (Freq: 28)\n",
      "Topic 92: migraine, pain, migraines, headache, banks (Freq: 28)\n",
      "Topic 93: satan, ra, god, heaven, bible (Freq: 27)\n",
      "Topic 94: summer, room, sublet, bedroom, dayton (Freq: 27)\n",
      "Topic 95: motif, devguide, linux, bindings, athena (Freq: 27)\n",
      "Topic 96: r12, air, substitutes, conditioning, heat (Freq: 27)\n",
      "Topic 97: freedom, beyer, speech, andi, aclu (Freq: 27)\n",
      "Topic 98: wax, scratches, plastic, paint, finish (Freq: 27)\n",
      "Topic 99: deskjet, ink, bubblejet, hp, printer (Freq: 26)\n",
      "Topic 100: station, redesign, space, option, options (Freq: 26)\n",
      "Topic 101: tape, backup, adaptec, 1542, aspi4dos (Freq: 26)\n",
      "Topic 102: des, key, attack, block, bits (Freq: 25)\n",
      "Topic 103: povray, tga, rle, pov, files (Freq: 25)\n",
      "Topic 104: car, dealer, price, sales, value (Freq: 25)\n",
      "Topic 105: seizures, corn, seizure, cereals, epilepsy (Freq: 24)\n",
      "Topic 106: stove, electric, napalm, dividian, mfrheinwpiwpiedu (Freq: 24)\n",
      "Topic 107: koresh, backing, biblical, b645zawutarlgutaedu, sermon (Freq: 24)\n",
      "Topic 108: neural, chemistry, molecular, 02106chopinudeledu, protein (Freq: 24)\n",
      "Topic 109: eye, dominance, prk, laser, rk (Freq: 24)\n",
      "Topic 110: evolution, theory, rawlins, fact, origins (Freq: 24)\n",
      "Topic 111: des, scicrypt, key, ciphers, cryptology (Freq: 24)\n",
      "Topic 112: 130, roads, road, speed, 80 (Freq: 24)\n",
      "Topic 113: chromium, weight, fat, diet, wa7kgx (Freq: 24)\n",
      "Topic 114: irq, interrupt, port, soundblaster, lpt1 (Freq: 23)\n",
      "Topic 115: solvent, adhesive, ducttape, mek, acetone (Freq: 23)\n",
      "Topic 116: mithras, osiris, masonry, oto, reuss (Freq: 23)\n",
      "Topic 117: uva, jefferson, partying, andi, beyer (Freq: 23)\n",
      "Topic 118: abortion, abortions, choice, pay, health (Freq: 23)\n",
      "Topic 119: ham, surges, power, interference, magnetic (Freq: 23)\n",
      "Topic 120: pillion, riding, advice, passenger, ride (Freq: 22)\n",
      "Topic 121: muslims, croats, serbs, bosnian, muslim (Freq: 22)\n",
      "Topic 122: accelerators, numlock, keyboard, emacs, keys (Freq: 22)\n",
      "Topic 123: midi, sound, driver, blaster, soundblaster (Freq: 22)\n",
      "Topic 124: crohns, ibd, inflammation, diet, disease (Freq: 22)\n",
      "Topic 125: leds, blue, boards, green, solder (Freq: 21)\n",
      "Topic 126: lens, camera, nikon, goldbergoasysdtnavymil, zoom (Freq: 21)\n",
      "Topic 127: ear, hearing, ears, earwax, dizziness (Freq: 20)\n",
      "Topic 128: maxaxaxaxaxaxaxaxaxaxaxaxaxaxax, mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v, pwisemansalmonusdedu, cliff, maxaxaxaxaxaxaxaxaxaxaxaxaxaxaxq (Freq: 20)\n",
      "Topic 129: wheelie, wheelies, shaftdrive, xlyxvax5citcornelledu, shaftdrives (Freq: 20)\n",
      "Topic 130: chain, wax, maxima, spooge, dod (Freq: 20)\n",
      "Topic 131: gas, tear, cs, riddle, nausea (Freq: 20)\n",
      "Topic 132: rosicrucian, amorc, tony, ch981clevelandfreenetedu, orders (Freq: 20)\n",
      "Topic 133: dialing, phones, tone, sweden, universal (Freq: 20)\n",
      "Topic 134: shots, allergy, rutin, nosebleeds, rhinitis (Freq: 20)\n",
      "Topic 135: sabbath, law, worship, ceremonial, jesus (Freq: 20)\n",
      "Topic 136: geico, insurance, claim, companies, accident (Freq: 20)\n",
      "Topic 137: mosques, tyre, jerusalem, mosque, israeli (Freq: 20)\n",
      "Topic 138: image, processing, analysis, plplot, data (Freq: 19)\n",
      "Topic 139: pregnency, teacher, biology, sperm, pregnancy (Freq: 19)\n",
      "Topic 140: saturn, dealer, profit, car, warranty (Freq: 19)\n",
      "Topic 141: odometer, sensor, mileage, car, odometers (Freq: 19)\n",
      "Topic 142: kidney, stones, she, calcium, b6 (Freq: 19)\n",
      "Topic 143: god, jesus, malcolm, punishable, loving (Freq: 19)\n",
      "Topic 144: joystick, joysticks, arcade, port, int15h (Freq: 19)\n",
      "Topic 145: energy, uranium, nuclear, oil, plutonium (Freq: 19)\n",
      "Topic 146: lyme, disease, ld, infectious, patients (Freq: 18)\n",
      "Topic 147: polio, syndrome, disease, postpolio, patients (Freq: 18)\n",
      "Topic 148: cancer, medical, circumcision, centers, center (Freq: 18)\n",
      "Topic 149: women, men, dariceyoyoccmonasheduau, sex, islamic (Freq: 18)\n",
      "Topic 150: 3do, list, 3d, rumours, lightwave (Freq: 18)\n",
      "Topic 151: orion, film, prototype, vacuum, leigh (Freq: 18)\n",
      "Topic 152: widgets, gadgets, widget, motif, pyeatttexacocom (Freq: 18)\n",
      "Topic 153: pat, accessdigexnet, express, online, communications (Freq: 18)\n",
      "Topic 154: engine, mr2, mr2s, noisy, eliot (Freq: 18)\n",
      "Topic 155: keys, des, key, 280, keyseach (Freq: 18)\n",
      "Topic 156: 42, tiff, philosophical, significance, joachimkihno (Freq: 18)\n",
      "Topic 157: gauge, gauges, wasps, temp, options (Freq: 17)\n",
      "Topic 158: dxf, iff, format, autocad, pei (Freq: 17)\n",
      "Topic 159: trinity, father, son, holy, spirit (Freq: 17)\n",
      "Topic 160: cooling, towers, nuclear, plants, water (Freq: 17)\n",
      "Topic 161: graphics, image, ray, 3d, rayshade (Freq: 17)\n",
      "Topic 162: cview, temp, directory, zenkar, mzmoscomcom (Freq: 17)\n",
      "Topic 163: comet, jupiter, gehrels, orbit, temporary (Freq: 17)\n",
      "Topic 164: icon, icons, program, manager, click (Freq: 17)\n",
      "Topic 165: eisa, isa, bus, vlb, motherboard (Freq: 17)\n",
      "Topic 166: xv, escaped, copyright, donation, author (Freq: 17)\n",
      "Topic 167: wire, wiring, ground, neutral, outlets (Freq: 17)\n",
      "Topic 168: lock, locks, cobra, kryptonite, cable (Freq: 17)\n",
      "Topic 169: adl, bullock, gerard, francisco, police (Freq: 17)\n",
      "Topic 170: scanner, scanners, logitech, scanman, price (Freq: 16)\n",
      "Topic 171: quadra, scsi, cartridge, mac, quadras (Freq: 16)\n",
      "Topic 172: beeps, chimes, machine, error, tj (Freq: 16)\n",
      "Topic 173: logo, startup, wincom, rle, windows (Freq: 16)\n",
      "Topic 174: nubus, pds, lc, slot, powerpc (Freq: 16)\n",
      "Topic 175: gun, buyback, guns, buy, accidental (Freq: 16)\n",
      "Topic 176: cosypak, mathematica, 8051, notebooks, directory (Freq: 16)\n",
      "Topic 177: cd, apple, cdrom, cd300is, applelink (Freq: 16)\n",
      "Topic 178: tires, tire, fluids, brake, braking (Freq: 16)\n",
      "Topic 179: parsli, quisling, norway, thomaspifiuiono, thomas (Freq: 16)\n",
      "Topic 180: duo, dock, apple, duodock, processor (Freq: 16)\n",
      "Topic 181: koresh, children, griffen, he, koreshs (Freq: 15)\n",
      "Topic 182: uv, bulb, flashlight, bulbs, light (Freq: 15)\n",
      "Topic 183: carbs, exhaust, intake, air, engine (Freq: 15)\n",
      "Topic 184: janet, reno, madman, she, children (Freq: 15)\n",
      "Topic 185: diesels, diesel, fuel, emissions, injector (Freq: 15)\n",
      "Topic 186: hacker, ethic, hackers, computer, dorsai (Freq: 15)\n",
      "Topic 187: yassin, deir, irgun, dir, village (Freq: 15)\n",
      "Topic 188: pope, schism, church, catholic, sspx (Freq: 15)\n",
      "Topic 189: sound, stereo, channel, mac, microphone (Freq: 15)\n",
      "Topic 190: vhs, kou, movie, 1100, movies (Freq: 15)\n",
      "Topic 191: duo, 230, crashes, apple, ram (Freq: 15)\n",
      "Topic 192: licensed, change, 2a42dubinskivmscsdmuedu, hex, underneth (Freq: 15)\n",
      "Topic 193: media, news, chomsky, cramer, passions (Freq: 14)\n",
      "Topic 194: luminosity, colour, hue, rgb, saturation (Freq: 14)\n",
      "Topic 195: brake, fluid, cylinder, master, clutch (Freq: 14)\n",
      "Topic 196: updating, winini, ini, svein, systemini (Freq: 14)\n",
      "Topic 197: drinking, riding, drink, drinks, sobriety (Freq: 14)\n",
      "Topic 198: hate, sin, love, sinner, our (Freq: 14)\n",
      "Topic 199: haldol, elderly, alzheimers, drugs, lithium (Freq: 14)\n",
      "Topic 200: file, gun, bill, firearms, brady (Freq: 14)\n",
      "Topic 201: shaft, wheelies, chain, shaftdrives, efficient (Freq: 14)\n",
      "Topic 202: bryce, bike, touring, overlook, arches (Freq: 14)\n",
      "Topic 203: jumper, 2190, maxtor, drive, jumpers (Freq: 14)\n",
      "Topic 204: het, de, een, van, vulcan (Freq: 14)\n",
      "Topic 205: easter, resurrection, celebration, pagan, goddess (Freq: 13)\n",
      "Topic 206: seema, varma, converter, ad, pcboard (Freq: 13)\n",
      "Topic 207: needles, acupuncture, needle, aids, hypodermic (Freq: 13)\n",
      "Topic 208: accelerations, acceleration, 45g, laxman, amruth (Freq: 13)\n",
      "Topic 209: temperature, sky, interstellar, radiation, dust (Freq: 13)\n",
      "Topic 210: floppy, drive, boot, disk, cmos (Freq: 13)\n",
      "Topic 211: spline, delaunay, triangulation, surface, developable (Freq: 13)\n",
      "Topic 212: vmax, handling, ba7116326ntuvaxntuacsg, handson, pls (Freq: 13)\n",
      "Topic 213: rocks, kids, erik, overpass, warningplease (Freq: 13)\n",
      "Topic 214: octopus, ice, detroit, zazula, octopuses (Freq: 13)\n",
      "Topic 215: fifth, keyphrase, amendment, key, passwords (Freq: 13)\n",
      "Topic 216: date, clock, dos, bios, menu (Freq: 13)\n",
      "Topic 217: font, fonts, dos, ssaunityncsuedu, alavi (Freq: 13)\n",
      "Topic 218: prophecies, lord, prophecy, earthquake, prophesies (Freq: 13)\n",
      "Topic 219: letter, lobby, letters, sammons, waycool (Freq: 13)\n",
      "Topic 220: movies, bikes, csundh30ursacalvinedu, sundheim, dod (Freq: 13)\n",
      "Topic 221: level, software, process, wingert, bret (Freq: 12)\n",
      "Topic 222: workspace, managers, workspaces, manager, workshift (Freq: 12)\n",
      "Topic 223: tongues, language, tounges, gifted, languages (Freq: 12)\n",
      "Topic 224: windy, wind, whenhow, yukyukyuk, castro (Freq: 12)\n",
      "Topic 225: selective, service, borden, abolish, reserve (Freq: 12)\n",
      "Topic 226: moment, silence, prayer, opposing, pray (Freq: 12)\n",
      "Topic 227: wrench, srb, thiokol, tool, pliers (Freq: 12)\n",
      "Topic 228: tape, copy, vcr, selfdestructing, tapes (Freq: 12)\n",
      "Topic 229: islamic, bcci, bank, gregg, jaegerbuphybuedu (Freq: 12)\n",
      "Topic 230: deadly, lowabiding, rocks, kids, heracleous (Freq: 12)\n",
      "Topic 231: weick, him, dana, cpu, sent (Freq: 12)\n",
      "Topic 232: sunset, sunrise, wetstein, times, jpwcbisecedrexeledu (Freq: 12)\n",
      "Topic 233: autotrace, illustrator, points, drawing, poorly (Freq: 11)\n",
      "Topic 234: cache, iisi, powercache, fpu, 32k (Freq: 11)\n",
      "Topic 235: mattingly, don, drm6640teslanjitedu, baseman, baseballalways (Freq: 11)\n",
      "Topic 236: 6174940565, fourdcom, dimension, cute, francescambensonfourdcom (Freq: 11)\n",
      "Topic 237: sword, malcolm, mormons, fc, whos (Freq: 11)\n",
      "Topic 238: weightlessness, sealevel, altitude, feel, cities (Freq: 11)\n",
      "Topic 239: eugenics, genes, memes, genome, we (Freq: 11)\n",
      "Topic 240: ampere, amp, db, company, ohmite (Freq: 11)\n",
      "Topic 241: tank, tankbag, zipper, fj11001200, blaine (Freq: 11)\n",
      "Topic 242: pgp, patent, license, patents, export (Freq: 10)\n",
      "Topic 243: swap, file, mathcad, windows, disk (Freq: 10)\n",
      "Topic 244: duo, educational, price, prices, apple (Freq: 10)\n",
      "Topic 245: dod, denizens, doom, muck, motorcycle (Freq: 10)\n",
      "Topic 246: pgp, distributable, freely, code, ee92jksbrunelacuk (Freq: 10)\n",
      "Topic 247: dumbest, stalk, lights, automotive, backup (Freq: 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install bertopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='all')['data']\n",
    "\n",
    "topic_model = BERTopic(nr_topics=\"auto\", calculate_probabilities=True, verbose=True)\n",
    "topics, _ = topic_model.fit_transform(data)\n",
    "\n",
    "topic_overview = topic_model.get_topic_freq()\n",
    "\n",
    "for topic_num, freq in topic_overview[1:].values:\n",
    "    topic_words = topic_model.get_topic(topic_num)\n",
    "    topic_summary = \", \".join([word[0] for word in topic_words[:5]])\n",
    "    print(f\"Topic {topic_num}: {topic_summary} (Freq: {freq})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA is efficient with sparse text data, requires fewer topics for readability, and often reveals nouns and adjectives in topics.\n",
    "\n",
    "LSA offers straightforward and distinct topics but may not capture complex interactions.\n",
    "\n",
    "lda2Vec supports hierarchical topic reduction and auto-determination of topic numbers but may not suit small datasets.\n",
    "\n",
    "BERTopic is versatile, stable across domains, supports hierarchical topics, but can generate many outliers.\n",
    "\n",
    "If you seek a balance between interpretability and performance, LDA is a suitable choice.\n",
    "\n",
    "For larger datasets with diverse content, BERTopic can be highly effective, assuming computational resources are available.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
